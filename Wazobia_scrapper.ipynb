{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vObclreojKjl"
      ],
      "authorship_tag": "ABX9TyN0a0o9UKot+DmmaejJPGVH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uchebuzz/wazobia_scrapper/blob/main/Wazobia_scrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scrapper"
      ],
      "metadata": {
        "id": "DFwlh_zxi-Hx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9qrSAlpogYv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wazobia FM Article Scraper\n",
        "# =========================\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from typing import List, Dict, Optional\n",
        "import re\n",
        "from dataclasses import dataclass, asdict\n",
        "import pandas as pd\n",
        "import yaml\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n",
        "\n",
        "# ## Load Configuration\n",
        "\n",
        "def load_config(config_path: str = \"config.yml\") -> dict:\n",
        "    \"\"\"Load configuration from YAML file\"\"\"\n",
        "    try:\n",
        "        with open(config_path, 'r', encoding='utf-8') as f:\n",
        "            return yaml.safe_load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Configuration file {config_path} not found\")\n",
        "        raise\n",
        "    except yaml.YAMLError as e:\n",
        "        print(f\"‚ùå Invalid YAML in configuration file: {e}\")\n",
        "        raise\n",
        "\n",
        "CONFIG = load_config()\n",
        "print(f\"‚úÖ Configuration loaded for {CONFIG['site_config']['name']}\")\n",
        "\n",
        "# ## Article Data Structure\n",
        "\n",
        "@dataclass\n",
        "class ArticleData:\n",
        "    \"\"\"Data structure for article information\"\"\"\n",
        "    title: str = \"\"\n",
        "    content: str = \"\"\n",
        "    author: str = \"\"\n",
        "    date: str = \"\"\n",
        "    url: str = \"\"\n",
        "    tags: List[str] = None\n",
        "    images: List[str] = None\n",
        "    scraped_at: str = \"\"\n",
        "    content_length: int = 0\n",
        "    category: str = \"\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.tags is None:\n",
        "            self.tags = []\n",
        "        if self.images is None:\n",
        "            self.images = []\n",
        "        if not self.scraped_at:\n",
        "            self.scraped_at = datetime.now().isoformat()\n",
        "        self.content_length = len(self.content)\n",
        "\n",
        "        # Extract category from URL\n",
        "        if self.url and not self.category:\n",
        "            url_parts = self.url.split('/')\n",
        "            categories = CONFIG['wazobia_fm_urls']['content_categories']\n",
        "            for part in url_parts:\n",
        "                if part in categories:\n",
        "                    self.category = part\n",
        "                    break\n",
        "\n",
        "    def to_dict(self):\n",
        "        return asdict(self)\n",
        "\n",
        "# ## Scraper Class\n",
        "\n",
        "class WazobiaScraper:\n",
        "    \"\"\"Wazobia FM article scraper\"\"\"\n",
        "\n",
        "    def __init__(self, config: dict = None):\n",
        "        self.config = config or CONFIG\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': self.config['site_config']['user_agent']\n",
        "        })\n",
        "        self.articles = []\n",
        "        self.scraped_urls = set()\n",
        "\n",
        "    def log_progress(self, message: str):\n",
        "        print(f\"üìä {message}\")\n",
        "\n",
        "    def get_page(self, url: str) -> Optional[BeautifulSoup]:\n",
        "        \"\"\"Fetch and parse a web page\"\"\"\n",
        "        try:\n",
        "            response = self.session.get(url, timeout=self.config['site_config']['timeout'])\n",
        "            response.raise_for_status()\n",
        "            time.sleep(self.config['site_config']['delay_between_requests'])\n",
        "            return BeautifulSoup(response.content, 'html.parser')\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"‚ùå Error fetching {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_nested_value(self, data, path):\n",
        "        \"\"\"Get nested value from dictionary using dot notation\"\"\"\n",
        "        keys = path.split('.')\n",
        "        value = data\n",
        "\n",
        "        for key in keys:\n",
        "            if isinstance(value, dict) and key in value:\n",
        "                value = value[key]\n",
        "            elif isinstance(value, list) and value and isinstance(value[0], dict) and key in value[0]:\n",
        "                value = value[0][key]\n",
        "            else:\n",
        "                return None\n",
        "        return value\n",
        "\n",
        "    def extract_from_json_ld(self, soup: BeautifulSoup) -> Optional[ArticleData]:\n",
        "        \"\"\"Extract article data from JSON-LD structured data\"\"\"\n",
        "        json_scripts = soup.find_all('script', {'type': 'application/ld+json'})\n",
        "\n",
        "        for script in json_scripts:\n",
        "            try:\n",
        "                json_content = script.string\n",
        "                if not json_content:\n",
        "                    continue\n",
        "\n",
        "                data = json.loads(json_content)\n",
        "                if isinstance(data, list):\n",
        "                    data = data[0] if data else {}\n",
        "\n",
        "                article_types = self.config['extraction_methods']['json_ld']['article_types']\n",
        "                if data.get('@type') not in article_types:\n",
        "                    continue\n",
        "\n",
        "                article_data = ArticleData()\n",
        "                field_mapping = self.config['extraction_methods']['json_ld']['field_mapping']\n",
        "\n",
        "                # Extract title\n",
        "                for path in field_mapping['title']:\n",
        "                    title = self.get_nested_value(data, path)\n",
        "                    if title:\n",
        "                        article_data.title = self.clean_text(str(title))\n",
        "                        break\n",
        "\n",
        "                # Extract content\n",
        "                for path in field_mapping['content']:\n",
        "                    content = self.get_nested_value(data, path)\n",
        "                    if content:\n",
        "                        article_data.content = self.clean_text(str(content))\n",
        "                        break\n",
        "\n",
        "                # Extract author\n",
        "                for path in field_mapping['author']:\n",
        "                    author = self.get_nested_value(data, path)\n",
        "                    if author:\n",
        "                        if isinstance(author, dict) and 'name' in author:\n",
        "                            article_data.author = self.clean_text(str(author['name']))\n",
        "                        else:\n",
        "                            article_data.author = self.clean_text(str(author))\n",
        "                        break\n",
        "\n",
        "                # Extract date\n",
        "                for path in field_mapping['date']:\n",
        "                    date = self.get_nested_value(data, path)\n",
        "                    if date:\n",
        "                        article_data.date = self.clean_text(str(date))\n",
        "                        break\n",
        "\n",
        "                # Extract images\n",
        "                for path in field_mapping['image']:\n",
        "                    image = self.get_nested_value(data, path)\n",
        "                    if image:\n",
        "                        if isinstance(image, list):\n",
        "                            article_data.images.extend([str(img) for img in image])\n",
        "                        else:\n",
        "                            article_data.images.append(str(image))\n",
        "                        break\n",
        "\n",
        "                # Extract tags\n",
        "                for path in field_mapping['tags']:\n",
        "                    tags = self.get_nested_value(data, path)\n",
        "                    if tags:\n",
        "                        if isinstance(tags, list):\n",
        "                            article_data.tags.extend([self.clean_text(str(tag)) for tag in tags])\n",
        "                        elif isinstance(tags, str):\n",
        "                            article_data.tags.extend([self.clean_text(tag.strip()) for tag in tags.split(',')])\n",
        "                        break\n",
        "\n",
        "                if article_data.title and article_data.content:\n",
        "                    return article_data\n",
        "\n",
        "            except (json.JSONDecodeError, KeyError, AttributeError):\n",
        "                continue\n",
        "\n",
        "        return None\n",
        "\n",
        "    def find_element_by_selectors(self, soup: BeautifulSoup, selectors: List[str]):\n",
        "        \"\"\"Find element using multiple CSS selectors\"\"\"\n",
        "        for selector in selectors:\n",
        "            try:\n",
        "                element = soup.select_one(selector)\n",
        "                if element:\n",
        "                    return element\n",
        "            except Exception:\n",
        "                continue\n",
        "        return None\n",
        "\n",
        "    def find_elements_by_selectors(self, soup: BeautifulSoup, selectors: List[str]):\n",
        "        \"\"\"Find elements using multiple CSS selectors\"\"\"\n",
        "        elements = []\n",
        "        for selector in selectors:\n",
        "            try:\n",
        "                found = soup.select(selector)\n",
        "                if found:\n",
        "                    elements.extend(found)\n",
        "            except Exception:\n",
        "                continue\n",
        "        return elements\n",
        "\n",
        "    def extract_from_html_selectors(self, soup: BeautifulSoup) -> Optional[ArticleData]:\n",
        "        \"\"\"Extract article data using HTML selectors\"\"\"\n",
        "        # Remove unwanted elements\n",
        "        for selector in self.config['content_filters']['exclude_selectors']:\n",
        "            for element in soup.select(selector):\n",
        "                element.decompose()\n",
        "\n",
        "        article_data = ArticleData()\n",
        "\n",
        "        # Extract title\n",
        "        title_element = self.find_element_by_selectors(soup, self.config['article_selectors']['title'])\n",
        "        if title_element:\n",
        "            article_data.title = self.clean_text(title_element.get_text())\n",
        "\n",
        "        # Extract content\n",
        "        content_elements = self.find_elements_by_selectors(soup, self.config['article_selectors']['content'])\n",
        "        content_parts = []\n",
        "        for element in content_elements:\n",
        "            text = self.clean_text(element.get_text())\n",
        "            if text and len(text) > 20:\n",
        "                content_parts.append(text)\n",
        "        article_data.content = ' '.join(content_parts)\n",
        "\n",
        "        # Extract author\n",
        "        author_element = self.find_element_by_selectors(soup, self.config['article_selectors']['author'])\n",
        "        if author_element:\n",
        "            article_data.author = self.clean_text(author_element.get_text())\n",
        "\n",
        "        # Extract date\n",
        "        date_element = self.find_element_by_selectors(soup, self.config['article_selectors']['date'])\n",
        "        if date_element:\n",
        "            article_data.date = self.clean_text(date_element.get_text())\n",
        "\n",
        "        # Extract tags\n",
        "        tag_elements = self.find_elements_by_selectors(soup, self.config['article_selectors']['tags'])\n",
        "        article_data.tags = [self.clean_text(tag.get_text()) for tag in tag_elements if tag.get_text().strip()]\n",
        "\n",
        "        # Extract images\n",
        "        img_elements = self.find_elements_by_selectors(soup, self.config['article_selectors']['images'])\n",
        "        for img in img_elements:\n",
        "            src = img.get('src')\n",
        "            if src:\n",
        "                full_img_url = urljoin(article_data.url or '', src)\n",
        "                article_data.images.append(full_img_url)\n",
        "\n",
        "        return article_data if article_data.title or article_data.content else None\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and normalize text content\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        text = ' '.join(text.split())\n",
        "        text = text.replace('&nbsp;', ' ').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')\n",
        "        text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\;\\:\\-\\(\\)\\[\\]\\\"\\']+', '', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def extract_article_links(self, soup: BeautifulSoup) -> List[str]:\n",
        "        \"\"\"Extract article links from page\"\"\"\n",
        "        links = []\n",
        "        link_elements = self.find_elements_by_selectors(soup, self.config['article_selectors']['article_links'])\n",
        "\n",
        "        for link in link_elements:\n",
        "            href = link.get('href')\n",
        "            if href:\n",
        "                full_url = urljoin(self.config['site_config']['base_url'], href)\n",
        "                if self.is_valid_article_url(full_url):\n",
        "                    links.append(full_url)\n",
        "\n",
        "        return list(set(links))\n",
        "\n",
        "    def is_valid_article_url(self, url: str) -> bool:\n",
        "        \"\"\"Check if URL is a valid article URL\"\"\"\n",
        "        try:\n",
        "            parsed = urlparse(url)\n",
        "            base_domain = urlparse(self.config['site_config']['base_url']).netloc\n",
        "\n",
        "            if parsed.netloc != base_domain:\n",
        "                return False\n",
        "\n",
        "            # Skip unwanted patterns\n",
        "            if any(pattern in url.lower() for pattern in self.config['content_filters']['skip_patterns']):\n",
        "                return False\n",
        "\n",
        "            # Only include valid content patterns\n",
        "            is_valid_content = any(pattern in url.lower() for pattern in self.config['content_filters']['valid_patterns'])\n",
        "\n",
        "            return is_valid_content and url not in self.scraped_urls\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def get_news_sections(self) -> List[str]:\n",
        "        \"\"\"Get all news section URLs\"\"\"\n",
        "        base_url = self.config['site_config']['base_url']\n",
        "        return [urljoin(base_url, section) for section in self.config['wazobia_fm_urls']['news_sections']]\n",
        "\n",
        "    def extract_article_data(self, url: str) -> Optional[ArticleData]:\n",
        "        \"\"\"Extract article data using multiple methods\"\"\"\n",
        "        soup = self.get_page(url)\n",
        "        if not soup:\n",
        "            return None\n",
        "\n",
        "        # Try extraction methods in priority order\n",
        "        for method in self.config['extraction_methods']['priority']:\n",
        "            if method == 'json_ld':\n",
        "                article_data = self.extract_from_json_ld(soup)\n",
        "                if article_data:\n",
        "                    print(f\"‚úÖ JSON-LD: {article_data.title[:50]}...\")\n",
        "                    break\n",
        "            elif method == 'html_selectors':\n",
        "                article_data = self.extract_from_html_selectors(soup)\n",
        "                if article_data and (article_data.title or len(article_data.content) > 100):\n",
        "                    print(f\"‚úÖ HTML: {article_data.title[:50]}...\")\n",
        "                    break\n",
        "            elif method == 'fallback':\n",
        "                title = soup.find('title')\n",
        "                if title:\n",
        "                    article_data = ArticleData()\n",
        "                    article_data.title = self.clean_text(title.get_text())\n",
        "                    paragraphs = soup.find_all('p')\n",
        "                    content_parts = [self.clean_text(p.get_text()) for p in paragraphs if len(p.get_text().strip()) > 50]\n",
        "                    article_data.content = ' '.join(content_parts)\n",
        "                    if article_data.content:\n",
        "                        print(f\"‚ö†Ô∏è  Fallback: {article_data.title[:50]}...\")\n",
        "                        break\n",
        "\n",
        "        if not article_data:\n",
        "            return None\n",
        "\n",
        "        article_data.url = url\n",
        "        content_length = len(article_data.content)\n",
        "        min_length = self.config['content_filters']['min_content_length']\n",
        "\n",
        "        if content_length < min_length:\n",
        "            print(f\"‚ö†Ô∏è  Content too short ({content_length} chars), skipping...\")\n",
        "            return None\n",
        "\n",
        "        return article_data\n",
        "\n",
        "    def scrape_articles(self, max_articles: int = 20, scrape_all_sections: bool = True) -> List[ArticleData]:\n",
        "        \"\"\"Main scraping method\"\"\"\n",
        "\n",
        "        if scrape_all_sections:\n",
        "            self.log_progress(\"Scraping from all Wazobia FM news sections...\")\n",
        "            all_links = []\n",
        "\n",
        "            news_sections = self.get_news_sections()\n",
        "            for section_url in news_sections[:5]:\n",
        "                self.log_progress(f\"Checking section: {section_url}\")\n",
        "                soup = self.get_page(section_url)\n",
        "                if soup:\n",
        "                    section_links = self.extract_article_links(soup)\n",
        "                    all_links.extend(section_links)\n",
        "                    self.log_progress(f\"Found {len(section_links)} articles in this section\")\n",
        "        else:\n",
        "            start_url = self.config['site_config']['base_url']\n",
        "            self.log_progress(f\"Starting scrape from {start_url}\")\n",
        "            soup = self.get_page(start_url)\n",
        "            if not soup:\n",
        "                print(f\"‚ùå Failed to fetch page: {start_url}\")\n",
        "                return []\n",
        "            all_links = self.extract_article_links(soup)\n",
        "\n",
        "        # Remove duplicates and limit\n",
        "        unique_links = list(set(all_links))\n",
        "        self.log_progress(f\"Found {len(unique_links)} unique article links\")\n",
        "\n",
        "        if len(unique_links) > max_articles:\n",
        "            unique_links = unique_links[:max_articles]\n",
        "\n",
        "        # Scrape individual articles\n",
        "        scraped_count = 0\n",
        "        for i, link in enumerate(unique_links, 1):\n",
        "            if scraped_count >= max_articles:\n",
        "                break\n",
        "\n",
        "            if link in self.scraped_urls:\n",
        "                continue\n",
        "\n",
        "            self.log_progress(f\"Scraping article {i}/{len(unique_links)}\")\n",
        "\n",
        "            article_data = self.extract_article_data(link)\n",
        "            if article_data and article_data.title:\n",
        "                self.articles.append(article_data)\n",
        "                self.scraped_urls.add(link)\n",
        "                scraped_count += 1\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è  Skipped: No valid content found\")\n",
        "\n",
        "        self.log_progress(f\"Scraping completed! Successfully scraped {scraped_count} articles\")\n",
        "        return self.articles\n",
        "\n",
        "    def get_dataframe(self) -> pd.DataFrame:\n",
        "        \"\"\"Convert articles to DataFrame\"\"\"\n",
        "        if not self.articles:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        data = [article.to_dict() for article in self.articles]\n",
        "        df = pd.DataFrame(data)\n",
        "        df['scraped_at'] = pd.to_datetime(df['scraped_at'])\n",
        "        return df\n",
        "\n",
        "    def save_to_json(self, filename: str = None) -> str:\n",
        "        \"\"\"Save articles to JSON file\"\"\"\n",
        "        if not filename:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            filename = f\"wazobia_fm_articles_{timestamp}.json\"\n",
        "\n",
        "        data = [article.to_dict() for article in self.articles]\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2, ensure_ascii=False, default=str)\n",
        "\n",
        "        print(f\"‚úÖ Data exported to {filename}\")\n",
        "        return filename\n",
        "\n",
        "    def save_to_csv(self, filename: str = None) -> str:\n",
        "        \"\"\"Save articles to CSV file\"\"\"\n",
        "        if not filename:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            filename = f\"wazobia_fm_articles_{timestamp}.csv\"\n",
        "\n",
        "        df = self.get_dataframe()\n",
        "\n",
        "        # Convert list columns to strings\n",
        "        df['tags'] = df['tags'].apply(lambda x: ', '.join(x) if x else '')\n",
        "        df['images'] = df['images'].apply(lambda x: ', '.join(x) if x else '')\n",
        "\n",
        "        df.to_csv(filename, index=False, encoding='utf-8')\n",
        "        print(f\"‚úÖ Data exported to {filename}\")\n",
        "        return filename\n",
        "\n",
        "print(\"‚úÖ WazobiaScraper class ready!\")\n",
        "\n",
        "\n",
        "# ## Ready to Use!\n",
        "print(\"\\nüéâ Wazobia FM Scraper Ready!\")\n",
        "print(\"üìñ Usage:\")\n",
        "print(\"   scraper = scrape_wazobia_fm(max_articles=20)\")\n",
        "print(\"   scraper.save_to_json()\")\n",
        "print(\"   scraper.save_to_csv()\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q_LKlIPgY7n",
        "outputId": "5eb955a8-8b06-4210-caf8-64f4d44a4309"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Libraries imported successfully!\n",
            "‚úÖ Configuration loaded for Wazobia FM\n",
            "‚úÖ WazobiaScraper class ready!\n",
            "\n",
            "üéâ Wazobia FM Scraper Ready!\n",
            "üìñ Usage:\n",
            "   scraper = scrape_wazobia_fm(max_articles=20)\n",
            "   scraper.save_to_json()\n",
            "   scraper.save_to_csv()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cuNA-DkDjo9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main_funtion"
      ],
      "metadata": {
        "id": "22ExakVXjpgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_wazobia_fm(max_articles=15):\n",
        "    \"\"\"Scrape articles from Wazobia FM\"\"\"\n",
        "    print(\"üìª Starting Wazobia FM Scraping\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    scraper = WazobiaScraper()\n",
        "    articles = scraper.scrape_articles(max_articles=max_articles)\n",
        "\n",
        "    if articles:\n",
        "        print(f\"\\nüìã Successfully scraped {len(articles)} articles\")\n",
        "\n",
        "        # Show sample articles\n",
        "        for i, article in enumerate(articles[:3], 1):\n",
        "            print(f\"\\n{i}. {article.title}\")\n",
        "            print(f\"   Author: {article.author or 'Not specified'}\")\n",
        "            print(f\"   Date: {article.date or 'Not specified'}\")\n",
        "            print(f\"   Category: {article.category or 'General'}\")\n",
        "            print(f\"   Content: {article.content[:150]}...\")\n",
        "\n",
        "        csv_file = scraper.save_to_csv()\n",
        "        print(f\"\\nüíæ Articles automatically saved to: {csv_file}\")\n",
        "\n",
        "        return scraper\n",
        "    else:\n",
        "        print(\"‚ùå No articles found\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "zzaLBArxjigU"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#call the function"
      ],
      "metadata": {
        "id": "ocAj_ANriPNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " scrape_wazobia_fm(max_articles=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynn12Ogug7dH",
        "outputId": "f73ef4b1-c66b-472b-e534-8ed8840e552f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìª Starting Wazobia FM Scraping\n",
            "==================================================\n",
            "üìä Scraping from all Wazobia FM news sections...\n",
            "üìä Checking section: https://www.wazobiafm.com/lagos/news/\n",
            "üìä Found 28 articles in this section\n",
            "üìä Checking section: https://www.wazobiafm.com/abuja/news/\n",
            "üìä Found 28 articles in this section\n",
            "üìä Checking section: https://www.wazobiafm.com/kano/news/\n",
            "üìä Found 28 articles in this section\n",
            "üìä Checking section: https://www.wazobiafm.com/onitsha/news/\n",
            "üìä Found 28 articles in this section\n",
            "üìä Checking section: https://www.wazobiafm.com/port-harcourt/news/\n",
            "üìä Found 28 articles in this section\n",
            "üìä Found 117 unique article links\n",
            "üìä Scraping article 1/1\n",
            "‚úÖ JSON-LD: PWAN Vs. Scott Iguma: Influencer Faces Trial Over ...\n",
            "üìä Scraping completed! Successfully scraped 1 articles\n",
            "\n",
            "üìã Successfully scraped 1 articles\n",
            "\n",
            "1. PWAN Vs. Scott Iguma: Influencer Faces Trial Over Alleged False Allegation\n",
            "   Author: Emeka Ezem\n",
            "   Date: 2025-07-17T14:59:1700:00\n",
            "   Category: General\n",
            "   Content: pp data-end\"438\" data-start\"216\"Iguma begin trend on social media since strong data-end\"495\" data-start\"480\"May 7, 2025strong after e post video for I...\n",
            "‚úÖ Data exported to wazobia_fm_articles_20250723_135236.csv\n",
            "\n",
            "üíæ Articles automatically saved to: wazobia_fm_articles_20250723_135236.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.WazobiaScraper at 0x7b396c9267d0>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ik4TUqeniQd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0AIKY73ziQ7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clean the data\n",
        "The data that is gotten from Wazobia is in a Jsonld, so its difficult to see the content. Here you just apply the function to the column"
      ],
      "metadata": {
        "id": "vObclreojKjl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y1pJ8sFgiRJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/wazobia_fm_articles_20250723_134533.csv\")\n",
        "df."
      ],
      "metadata": {
        "id": "oRwdUl_HiRVw"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}